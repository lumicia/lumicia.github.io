---
title: "程序员的自我修养 Chap1：温故而知新"
date: 2021-05-14T16:59:40+08:00
categories: ["Compiler Principle"]
tags: ["程序员的自我修养：装载、链接与库"]
draft: false
---

## 回顾 Hello World

回顾 C 语言编写的 Hello World 程序：

```c
#include <stdio.h>

int main() {
    printf("Hello World\n");
    return 0;
}
```

<!--more-->

仔细思考以下问题：

1. 程序为什么要被编译器编译了之后才可以运行？
2. 编译器在把 C 语言程序转换称可以执行的机器码的过程中做了什么，怎么做的？
3. 最后编译出来的可执行文件里面是什么？除了机器码还有什么？它们是怎么存放的，怎么组织的？
4. `#include <stdin.h>` 是什么意思？把 `stdio.h` 包含进来意味着什么？C 语言库又是什么？它是怎么实现的？
5. 不同的编译器和不同的硬件平台以及不同的操作系统，最终编译的结果一样吗？为什么？
6. Hello World 程序是怎么运行起来的？操作系统是怎么装载它的？它从哪儿开始执行，到哪儿结束？`main` 函数之前发生了什么，结束之后又发生了什么？
7. 如果没有操作系统，Hello World 程序可以运行吗？如果在一台没有操作系统的机器上运行 Hello World 程序需要什么？应该怎么实现？
8. `printf` 是怎么实现的？它为什么可以有不定数量的参数？为什么它能够在终端上输出字符串？
9. 运行过中的 Hello World 程序在内存中是什么样子的？

## 计算机硬件

对于程序员来说计算机中最重要的三个部件是：CPU、内存和 I/O 控制芯片。

早期 CPU 核心频率不高，跟内存的频率一样，它们直接连接在同一条总线（bus）上。

I/O 设备速度比 CPU 和内存慢很多，为了协调 I/O 设备与总线之间的速度，也为了能让 CPU 和 I/O 设备直接通信，一般每个设备都会有一个相应的 I/O 控制器。

CPU 核心频率提升之后，内存跟不上 CPU 的速度，于是产生了与内存频率一致的系统总线，而 CPU 采用倍频的方式与系统总线进行通信。

为了协调 CPU、内存和高速的图形设备，专门设计了高速的北桥芯片，使它们之间能高速交换数据。另外为了低速设备设计了南桥芯片，南桥芯片汇总之后连接到北桥上。系统总线采用 PCI 结构，低速设备采用 ISA 总线。

> :spiral_notepad: 现在没有北桥和南桥了。

PCI 最高速度为 133 MHz，还是不能满足需求，于是有了 PCI Express 结构。

单个 CPU 频率提升开始放缓后，为了提高计算机运行速度，开始增加 CPU 数量。一种形式是对称多处理器（SMP，Symmetrical Multi-Processing）。

将多个处理器合并成一个 CPU，称为多核处理器（Multi-core Processor）。

## 系统软件和 API

一般将用于管理计算机本身的软件称为系统软件，以区别于普通的应用程序。

系统软件可分为平台性（如操作系统内核、驱动程序、运行库和系统工具）和程序开发性的（如编译器、汇编器、链接器等开发工具和开发库）。

计算机系统体系结构是分层的，层与层之间的通信协议称为接口（Interface）。接口下层是接口的提供者，由它定义接口；接口上层是接口的使用者，利用接口实现所需的功能。硬件和应用程序之间都是中间层。

应用程序使用的接口称为应用程序编程接口（API，Application Programming Interface）。API 的提供者是运行库，如 Linux 下的 Glibc 提高 POSIX 的 API，Windows 的运行库提供 Windows 的 API，32 位 Windows 提供的 API 称为 Win32。

运行库使用操作系统提供的系统调用接口（System call Interface），系统调用接口在实现中往往以软件中断（Software Interrupt）的方式提供。如 Linux 使用 `0x80` 号中断作为系统调用接口，Windows 使用 `0x2E` 号中断作为系统调用接口。

> :spiral_notepad: Windows 的系统调用方式已改变。

操作系统内核层对于硬件层来说是硬件接口的使用者，而硬件是接口的定义者，硬件的接口决定了操作系统内核，具体来讲就是驱动程序如何操作硬件，如何与硬件通信。这种接口往往称为硬件规格（Hardware Specification），硬件的生产厂商负责提供硬件规格，操作系统和驱动程序的开发者通过阅读硬件规格文档所规定的各种硬件编程接口来编写操作系统和驱动程序。

## 操作系统

操作系统的一个功能是提供抽象的接口，另一个主要功能是管理硬件资源。

早期当 CPU 未被程序使用时，监控程序启动其他等待 CPU 资源的程序，来充分利用 CPU。这种技术称为多道程序（Multiprogramming）。多道程序技术成功提高了 CPU 的利用率，但调度策略太粗糙，无法为急需 CPU 的程序（如用户交互）及时分配 CPU。

于是出现一种协作的模式，每个程序运行一段时间后主动让出 CPU 给其他程序，使一段时间内每个程序都可以运行一小段时间。这使得一些交互式的任务，如点击鼠标或按下键盘按键，能立即得到处理，用户看得到效果。这种程序协作模式称为分时系统（Time-Sharing System）。监控程序慢慢形成操作系统的雏形。

随着硬件发展，出现了多任务（Multi-tasking）系统，操作系统接管所有硬件资源，并且本身运行在一个受硬件包含的级别。所有应用都以进程（Process）的方式运行在比操作系统权限更低的级别，每个进程都有自己独立的地址空间，进程的地址空间相互隔离开。CPU 由操作系统统一进行分配，每个进程根据进程优先级高低来分配 CPU 资源。进程运行超过一定时间后，操作系统暂停该进程，将 CPU 资源分配给其他等待运行的进程。

这种 CPU 的分配方式即所谓的抢占式（Preemptive），操作系统可以强制将 CPU 资源分配给它认为目前最需要的进程。如果操作系统分配给每个进程的时间都很短，CPU 在多个进程之间快速切换，从而造成很多进程都在同时运行的现象。

操作系统成熟以后，硬件逐渐被抽象为一系列概念，繁琐的硬件细节都交给操作系统，由操作系统中的硬件驱动（Device Driver）程序来完成。驱动程序可以看作操作系统的一部分，往往跟操作系统内核一起运行在特权级。但驱动程序与操作系统内核之间又有一定独立性，从而得到比较好的灵活性。驱动程序通常由硬件生产厂商开发，操作系统开发者为硬件生产厂商提供了一系列接口和框架。

## 虚拟内存

将计算机上有限的物理内存直接分配给多个程序使用的问题：

- 地址空间不隔离；
- 内存使用效率低；
- 程序运行的地址不确定。

解决方法是把程序地址视为虚拟地址（Virtual Address），然后通过将虚拟地址映射为实际的物理地址。只要能妥善控制虚拟地址到物理地址的映射过程，就可以保证任意一个程序能访问的物理内存区域跟其他程序不重叠，从而隔离地址空间。

可以把地址空间看成一个很大的数组，每个数组的元素是一个字节，而这个数组大小由地址空间的地址长度决定。

地址空间分为虚拟地址空间（Virtual Address Space）和物理地址空间（Physical Address Space）。物理地址空间是实际存在于计算机中的。虚拟地址空间是虚拟的，每个进程都有自己独立的虚拟空间，而且每个进程只能访问自己的地址空间，从而做到进程隔离。

虚拟地址空间到物理地址空间的映射，最开始使用的方法是分段（Segmentation）。把一段与程序所需要的内存空间大小的虚拟空间映射到某个地址空间。先假设一个地址，然后从实际的物理内存中分配一个相同大小的物理地址，把这两块相同大小的地址空间一一映射，即虚拟空间中的每个字节对应于物理空间中的每个字节。映射过程由软件来设置，实际的地址转换由硬件来完成。

分段未能解决内存使用效率的问题。为了提高内存的使用率，需要更小粒度的内存分割和映射的方法，于是出现了分页（Paging）。

分页的基本原理是把地址空间等分成固定大小的页，页的大小由硬件决定，或硬件支持多种大小的页，由操作系统选择决定页的大小。通常使用 4KB 大小的页。虚拟空间的页称为虚拟页（VP，Virtual Page），物理内存中的页称为物理页（PP，Physical Page），磁盘中的页称为磁盘页（DP，Disk Page）。

进程需要的页不再内存中时，硬件捕获这个消息，称为页错误（Page Fault），然后操作系统接管进程，负责从磁盘中读取所需的页并装入内存，将内存中的页和磁盘页建立映射关系。

保护也是页映射的目的之一。每个页可以设置权限属性（读和写），只有操作系统由权限修改这些属性，因此操作系统可以做到保护自己和进程。

虚拟存储的实现需要硬件支持，通常采用 MMU（Memory Management Unit）部件来进行页映射。一般 MMU 集成在 CPU 内部。

```mermaid
graph LR
	id1(CPU) --虚拟地址--> id2(MMU) --物理地址--> id3(物理内存)
```

## 线程

CPU 向多核发展，于是出现多线程，是实现软件并发执行的重要方式。

线程（Thread）有时称为轻量级进程（LWP，Lightweight Process），是程序执行流的最小单元。一个标准的线程由线程 ID、当前指令指针（PC）、寄存器集合和堆栈组成。

通常一个进程由一个到多个线程组成，各个线程之间共享程序的内存空间（包括代码段、数据段、堆等）及一些进程级的资源（如打开文件和信号）。

多线程的优点：

- 某个操作可能会陷入长时间等待，等待的线程会进入睡眠状态，无法继续执行。多线程执行可以有效利用等待的时间，例如等待网络响应。
- 某个操作会消耗大量时间，如果只有一个线程，程序和用户之间的交互会中断。多线程可以让一个线程负责交互，其他线程负责计算。
- 程序逻辑本身要求并发操作，如一个多端下载软件。
- 多 CPU 或多核计算机，本身具备同时执行多个线程的能力。
- 多线程比多进程在数据共享方面效率高很多。

线程可以访问进程内存中的所有数据，包括其他线程的堆栈，同时线程也有自己的私有存储空间：

- 栈；
- 线程局部存储（TLS，Thread Local Storage）；
- 寄存器。

当线程数量小于等于处理器数量时，线程的并发是真正的并发。在单处理器中的多线程是一种模拟出来的状态。操作系统让多线程程序轮流执行，这种切换行为称为线程调度（Thread Schedule）。

线程调度中，线程拥有至少三种状态：

- 运行（Running）：线程正在执行；
- 就绪（Ready）：线程可以立刻运行，但 CPU 已经被占用；
- 等待（Waiting）：线程正在等待某一事件（I/O 或同步）发生，无法执行。

处于运行中线程拥有一段可以执行的时间，称为时间片（Time Slice）。当时间片用尽时，进程会进入就绪状态。如果在时间片用尽之前就开始等待某事件，那么它将进入等待状态。每个一个线程离开运行状态时，调度系统就会选择一个其他的就绪状态线程继续执行。在一个处于等待状态的线程所等待的事件发生之后，该线程将进入就绪状态。

<img src="https://i.loli.net/2021/05/15/YOgVijqt5J9H7kT.png" alt="线程状态切换" style="zoom:50%;" />

线程调度方式：

- 优先级调度（Priority Schedule）：决定线程按照什么顺序轮流执行。线程拥有各自的线程优先级（Thread Priority），高优先级的线程会更早执行，所有高优先级线程执行完成后再执行低优先级的线程。
- 轮转法（Round Robin）：让各个线程轮流执行一小段时间，线程之间交错执行。

在 Windows 中使用 `BOOL WINAPI SetThreadPriority(HANDLE hThread, int nPriority)` 来设置线程优先级。

在 Linux 中使用 `pthread` 库来实现与线程相关的操作。

除手动设置以外，系统可以根据不同的线程表现自动调整线程的优先级，使调度更有效率。

频繁等待的线程称为 IO 密集型线程（IO Bound Thread），频繁计算而很少等待的线程称为 CPU 密集型线程（CPU Bound Thread）。IO 密集型线程比 CPU 密集型线程更容易得到优先级的提升。

在优先级调度下，线程存在饿死（Starvation）现象：较低级别的线程执行之前，总是有较高优先级的线程试图执行，因此这个低优先级线程始终无法执行。

当一个 CPU 密集型的线程获得较高的优先级时，许多低优先级的进程就可能饿死。而一个高优先级的 IO 密集型线程由于大部分时间都处于等待状态，相对不容易造成其他线程饿死。

为了避免饿死现象，调度系统常常会逐步提升那些等待了过长时间得不到执行的线程的优先级。这样一个线程只要等待足够长的时间，其优先级一定会提高到足够让它执行的程度。

因此在优先级的调用环境下，线程的优先级改变有三种方式：

1. 用户指定优先级；
2. 根据进入等待状态的频繁程度提升或降低优先级；
3. 长时间得不到执行而被提升优先级。

线程在用尽时间片之后，会被操作系统强制进入就绪状态，这个过程称为抢占（Preemption），因为之后执行的线程抢占了当前线程。

Windows 内核有明确有进程和线程的概念，有明确的 `CreateProcess` 和 `CreateThread` API 来创建进程和线程，并有一系列的 API 来操纵它们。

Linux 不存在真正意义上的线程。Linux 将所有的执行实体（无论是线程还是进程）都称为任务（Task），每一个任务概念上类似于一个单线程的进程，具有内存空间、执行实体、文件资源等。但是 Linux 下不同的任务之间可以选择共享内存空间，因此共享内存空间的任务实际构成了一个进程，这些任务也就成了这个进程中的线程。

Linux 创建新的任务的系统调用：

- `fork`：复制当前进程；
- `exec`：使用新的可执行映像覆盖当前可执行映像；
- `clone`：创建子进程并从指定位置开始执行。

`fork` 函数会产生一个和当前进程完全一样的新进程，并和当前进程一样从 `fork` 函数里返回。

```c
pid_t pid;
if (pid = fork()) {...}
```

`fork` 函数调用之后，新的任务将启动并和原来的任务一起从 `fork` 函数返回。原来的任务 `fork` 后返回新任务 `pid`，而新任务的 `fork` 返回 `0`。

`fork` 产生新任务的速度非常快，因为 `fork` 并不复制原任务的内存空间，而是和原任务一起共享一个写时复制（COW，Copy on Write）的内存空间。写时复制指两个任务可以同时自由地读取内存，但任意一个任务试图修改内存时，内存会复制一份给修改方单独使用，以免影响到其他的任务使用。

<img src="https://i.loli.net/2021/05/15/I3arEXqYCgvkUut.png" alt="Copy-on-Write.png" style="zoom:50%;" />

`fork` 只能产生原任务的镜像，必须配合 `exec` 来启动其他新任务。`exec` 可以用新的可执行映像替换当前的可执行映像，因此在 `fork` 产生一个新任务之后，新任务可以调用 `exec` 来执行新的可执行文件。

如果要产生新线程，可以使用 `clone`。`clone` 产生新任务之后，从指定位置开始执行，并且（可选的）共享当前进程的内存空间和文件等。这样就实际达到了线程的效果。

`clone` 函数的原型：

```c
int clone(int (*fn)(void*), void* child_stack, int flags, void* arg);
```

### 线程安全

多线程程序中可访问的全局变量和堆数据随时都可能被其他线程改变。因此多线程程序在并发时数据的一致性非常重要。

多个线程同时访问一个共享变量，后果是难以预测的。假设有一个变量 `i = 1`。线程 1 执行 C 代码：

```c
++i;
```

线程 2 执行：

```c
--i;
```

在许多体系结构上，`++i` 的实现方式为：

1. 读取 `i` 的值到某个寄存器 `X`；
2. `X++`；
3. 将 `X` 的内容存储回 `i`。

由于线程 1 和线程 2 并发执行，因此两个线程的执行序列可能如下表：

| 执行序号 | 执行指令 | 语句执行后变量的值   | 线程 |
| -------- | -------- | -------------------- | ---- |
| 1        | `i = 1`  | `i = 1, X1 = UNKNOW` | 1    |
| 2        | `X1 = 1` | `i = 1, X1 = 1`      | 1    |
| 3        | `X2 = i` | `i = 1, X2 = 1`      | 2    |
| 4        | `X1++`   | `i = 1, X1 = 2`      | 1    |
| 5        | `X2--`   | `i = 1, X2 = 0`      | 2    |
| 6        | `i = X1` | `i = 2, X1 = 2`      | 1    |
| 7        | `i = X2` | `i = 0, X2 = 0`      | 2    |

从程序逻辑来看，两个线程都执行完毕之后，`i` 的值应该为 `1`，但从之前的执行序列可以看到，`i` 得到的值是 `0`。实际这两个线程如果同时执行的话，`i` 的结果可能是 `0` 或 `1` 或 `2`。

自增操作 `++` 在多线程环境下会出现错误是因为被编译为汇编代码后不止一条指令，因此在执行的时候可能执行了一半就被调度系统打断，去执行其他代码。我们把单指令的操作称为原子的（Atomic），因此单条指令的操作无论如何都不会被打断。

在 Windows 中进行原子操作的 API 称为 Interlocked API。

| Windows API          | 作用               |
| -------------------- | ------------------ |
| InterlockedExchange  | 原子地交换两个值   |
| InterlockedDecrement | 原子地减少一个值   |
| InterlockedIncrement | 原子地增加一个值   |
| InterlockedXor       | 原子地进行异或运算 |

尽管原子操作指令非常方便，但只适用于比较简单的场合。在更复杂的场合需要更通用的方法：锁。

为了避免多个线程同时读写同一个数据而产生不可预料的后果，需要将各个线程对同一个数据的访问同步（Synchronization）。同步指在一个线程访问数据未结束时，其他线程不能对同一个数据进行访问。这样，对数据的访问被原子化了。

同步最常用的方法是锁（Lock）。锁是一种非强制机制，每一个线程在访问数据或资源之前首先试图获取（Acquire）锁，在访问结束之后释放（Release）锁。在锁已经被占用的时候试图获取锁时，线程会一直等待，直到锁重新可用。

二元信号量（Binary Semaphore）是最简单的一种锁，只有两种状态：占用与非占用。它适合只能被唯一一个线程独占访问的资源。当二元信号量处于非占用状态时，第一个试图获取该二元信号量的线程会获得该锁，并将二元信号量置为占用状态，此后其他的所有试图获取该二元信号量的线程将会等待，直到该锁被释放。

对于允许多个线程并发访问的资源，选择多元信号量，简称信号量（Semaphore）或信标。一个初始值为 `N` 的信号量允许 N 个线程并发访问。线程访问资源的时候首先获取信号量，进行如下操作：

1. 将信号量的值减 1；
2. 如果信号量的值小于 0，则进入等待状态，否则继续执行。

访问完资源之后，线程释放信号量，进行如下操作：

1. 将信号量的值加 1；
2. 如果信号量的值小于 1，唤醒一个等待中的线程。

互斥量（Mutex）类似于二元信号量，资源仅同时允许一个线程访问。但信号量在整个系统可以被任意线程获取并释放，也就是说，同一个信号量可以被系统中的一个线程获取之后由另一个线程释放。而互斥量则要求获取了互斥量的线程负责释放这个锁，其他线程释放互斥量是无效的。

临界区（Critical Section）是比互斥量更加严格的同步方法。临界区的锁的获取称为进入临界区，锁的释放称为离开临界区。临界区与互斥量、信号量的区别在于，互斥量和信号量在系统的任何进程里都是可见的，一个进程创建了一个在互斥量或信号量，另一个进程试图去获取该锁是合法的。然而，临界区的作用范围仅限于本进程，其他进程无法获取该锁。除此之外，临界区和互斥量具有相同的性质。

读写锁（Read-Write Lock）用于更加特定场合的同步。对于频繁读取，仅偶尔写入的情况，信号量、互斥量和临界区都非常低效，因此出现了读写锁来避免这个问题。对于同一个锁，读写锁有两种获取方式，共享的（Shared）或独占的（Exclusive）。

- 当锁处于自由状态时，线程以任何一种方式获取锁都能成功，并将锁置于对应的状态。
- 如果锁处于共享状态，其他线程以共享的方式获取锁仍能成功，此时这个锁分配给了多个线程。然而，如果其他线程试图以独占的方式获取已经处于共享状态的锁，那么它必须等待锁被所有的线程释放。
- 相应地，处于独占状态的锁将阻止任何其他线程获取该锁，不论它们试图以哪种方式获取。

| 读写锁状态 | 以共享的方式获取 | 以独占的方式获取 |
| ---------- | ---------------- | ---------------- |
| 自由       | 成功             | 成功             |
| 共享       | 成功             | 等待             |
| 独占       | 等待             | 等待             |

条件变量（Condition Variable）也是一种同步手段，作用类似于一个栅栏。

- 线程可以等待条件变量，一个条件变量可以被多个线程等待。
- 线程可以唤醒条件变量，此时某个或所有等待此条件变量的线程都会被唤醒并继续支持。

条件变量可以让许多线程一起等待某个事件方式，当事件发生时（条件变量被唤醒），所有的线程可以一起恢复执行。

一个函数被重入，表示这个函数没有执行完成，由于外部因素或内部调用，又一次进入该函数执行。重入一个函数，只有两种情况：

- 多个线程同时执行这个函数；
- 函数自身（可能是经过多层调用之后）调用自身。

一个函数被称为可重入的（Reentrant），表明该函数被重入之后不会产生任何不良后果。可重入的函数具有如下特点：

- 不使用任何（局部）静态或全局的非 `const` 变量；
- 不返回任何（局部）静态或全局的非 `const` 变量的指针；
- 仅依赖于调用方提供的参数；
- 不依赖任何单个资源的锁（mutex 等）；
- 不调用任何不可重入的函数。

可重入是并发安全的强力保障，一个可重入的函数在多线程环境下可以放心使用。

CPU 过去发展出了动态调度，在执行程序的时候为了提高效率可能交换指令的顺序。同样，编译器在进行优化的时候，可能为了效率而交换毫不相干的两条相邻指令的执行顺序。

可以使用 `volatile` 关键字试图阻止过度优化。`volatile` 可以做到两件事：

1. 阻止编译器为了提高速度将一个变量缓存到寄存器内而不写回；
2. 阻止编译器调整操作 `volatile` 变量的指令顺序。

`volatile` 能阻止编译器调整指令顺序，但无法阻止 CPU 动态调度调整指令顺序。

通常调用 CPU 提供的 `barrier` 指令来阻止 CPU 将该指令之前的指令交换到 `barrier` 之后，反之亦然。

## 多线程内部

线程的并发执行是由多处理器或操作系统调度来实现的。

内核线程是如此，但用户实际使用的线程是存在于用户态的用户线程。用户线程不一定在操作系统内核对应等同数量的内核线程，如某些轻量级的线程库。

用户态多线程库的三种模型：

- 一对一模型；
- 多对一模型；
- 多对多模型。

### 一对一模型

对于直接支持线程的系统，一对一始终是最简单的模型。一个用户使用的线程对应于一个内核使用的线程（但一个内核的线程不一定有对应的用户态线程存在）。

优点是用户线程和内核线程一致，线程之间的并发是真正的并发，一个线程因为某些原因阻塞时，其他线程的执行不会受到影响。此外，一对一模型也可以让多线程程序在多处理器的系统上有更好的表现。

一般直接使用 API 或系统调用创建的线程均为一对一线程。如 Linux 里使用 `clone`（带有 `CLONE_VM` 参数）产生的线程就是一对一线程。

```c
int thread_funciton(void*) {...}
char thread_stack[4096];

void foo {
    clone(thread_function, thread_stack, CLONE_VM, 0);
}
```

在 Windows 里使用 `CreateThread` API 创建一对一的线程。

一对一模型的缺点：

1. 由于许多操作系统限制了内核线程的数量，因此一对一线程会让用户线程的数量也受到限制；
2. 许多操作系统内核线程调度时，上下文切换的开销较大，导致用户线程的执行效率降低。

### 多对一模型

多对一模型将多个用户线程映射到一个内核线程上，线程之间的切换由用户态的代码来进行，因此线程切换的速度比一对一模型快很多。

多对一模型的一大问题是，如果其中一个用户线程阻塞，那么所有的线程将无法执行，因为此时内核里的线程也随之阻塞了。

另外，在多处理器系统上，处理器的增多对多对一模型的线程性能没有明显帮助。但同时，多对一模型可以得到高效的上下文切换和几乎无限制的线程数量。

### 多对多模型

多对多模型将多个用户线程映射到少数几个内核线程上。

多对多模型中，一个用户线程阻塞并不会使得所有的用户线程阻塞，因为此时还有别的线程可以被调用来执行。另外，多对多模型对用户线程的数量也没有什么限制，在多处理器系统上，多对多模型的线程也能得到一定的性能提升，但幅度不如一对一模块高。